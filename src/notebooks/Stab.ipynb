{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb728412",
   "metadata": {},
   "source": [
    "Importação da etapa de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6cbd324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../STab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7123d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import ChurnDataProcessor\n",
    "from STab import MainModel, Num_Cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f194e6",
   "metadata": {},
   "source": [
    "Importação das bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc68042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.stats import ks_2samp\n",
    "import keras4torch\n",
    "from keras4torch.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7dfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de semente para reprodutibilidade\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.abspath('../../data/customer_churn_telecom_services.csv')\n",
    "processor = ChurnDataProcessor(file_path)\n",
    "processor.split_and_balance() # realizar apenas a divisão dos dados, porque vamos usar técnicas de tratamento diferentes para o STab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log de verificação dos dados\n",
    "print(f\"Dados carregados e divididos.\")\n",
    "print(f\"Treino: {processor.train_df.shape}\")\n",
    "print(f\"Validação: {processor.validation_df.shape}\")\n",
    "print(f\"Teste: {processor.test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_stab(processor):\n",
    "    \"\"\"\n",
    "    Prepara os dados do processor para o formato específico do STab:\n",
    "    - Numéricas: StandardScaler\n",
    "    - Categóricas: LabelEncoder (Inteiros)\n",
    "    - Retorno: Listas [X_num, X_cat] compatíveis com Num_Cat\n",
    "    \"\"\"\n",
    "    # Cópias para não alterar o original\n",
    "    train_df = processor.train_df.copy()\n",
    "    val_df = processor.validation_df.copy()\n",
    "    test_df = processor.test_df.copy()\n",
    "\n",
    "    # Tratamento de Nulos\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['TotalCharges'] = df['TotalCharges'].fillna(0.0)\n",
    "\n",
    "    # Definição de Colunas\n",
    "    target_col = 'Churn'\n",
    "    num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    # Todas as outras colunas (exceto target e numéricas) são categóricas\n",
    "    cat_cols = [c for c in train_df.columns if c not in num_cols + [target_col]]\n",
    "\n",
    "    # 1. Processamento Numérico (StandardScaler)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(train_df[num_cols]).astype(np.float32)\n",
    "    X_val_num = scaler.transform(val_df[num_cols]).astype(np.float32)\n",
    "    X_test_num = scaler.transform(test_df[num_cols]).astype(np.float32)\n",
    "\n",
    "    # 2. Processamento Categórico (LabelEncoder)\n",
    "    cat_cardinalities = []\n",
    "    \n",
    "    # Matrizes para guardar os índices inteiros\n",
    "    X_train_cat = np.zeros((len(train_df), len(cat_cols)), dtype=np.int64)\n",
    "    X_val_cat = np.zeros((len(val_df), len(cat_cols)), dtype=np.int64)\n",
    "    X_test_cat = np.zeros((len(test_df), len(cat_cols)), dtype=np.int64)\n",
    "\n",
    "    for i, col in enumerate(cat_cols):\n",
    "        le = LabelEncoder()\n",
    "        # Treina com todos os dados possíveis para não dar erro de categoria desconhecida\n",
    "        all_data = pd.concat([train_df[col], val_df[col], test_df[col]])\n",
    "        le.fit(all_data.astype(str))\n",
    "        \n",
    "        X_train_cat[:, i] = le.transform(train_df[col].astype(str))\n",
    "        X_val_cat[:, i] = le.transform(val_df[col].astype(str))\n",
    "        X_test_cat[:, i] = le.transform(test_df[col].astype(str))\n",
    "        \n",
    "        # Guarda a cardinalidade (+1 para segurança)\n",
    "        cat_cardinalities.append(len(le.classes_) + 1)\n",
    "\n",
    "    # 3. Targets\n",
    "    le_target = LabelEncoder()\n",
    "    y_train = torch.tensor(le_target.fit_transform(train_df[target_col]), dtype=torch.long)\n",
    "    y_val = torch.tensor(le_target.transform(val_df[target_col]), dtype=torch.long)\n",
    "    y_test = torch.tensor(le_target.transform(test_df[target_col]), dtype=torch.long)\n",
    "\n",
    "    return (\n",
    "        [X_train_num, X_train_cat], y_train,\n",
    "        [X_val_num, X_val_cat], y_val,\n",
    "        [X_test_num, X_test_cat], y_test,\n",
    "        cat_cardinalities,\n",
    "        len(num_cols)\n",
    "    )\n",
    "\n",
    "# Executa a preparação\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, categories_list, num_continuous = prepare_data_for_stab(processor)\n",
    "\n",
    "print(f\"Dados transformados para STab.\")\n",
    "print(f\"Cardinalidades das categorias: {categories_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b79272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- Espaço de Busca ---\n",
    "    params = {\n",
    "        'dim': trial.suggest_categorical('dim', [8, 16, 32, 64]),\n",
    "        'depth': trial.suggest_int('depth', 1, 6),\n",
    "        'heads': trial.suggest_categorical('heads', [2, 4, 8]),\n",
    "        'attn_dropout': trial.suggest_float('attn_dropout', 0.0, 0.5),\n",
    "        'ff_dropout': trial.suggest_float('ff_dropout', 0.0, 0.5),\n",
    "        'U': trial.suggest_int('U', 1, 4), \n",
    "        'cases': trial.suggest_categorical('cases', [8, 16]),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    }\n",
    "\n",
    "    # Instanciando o Modelo Base (MainModel)\n",
    "    base_model = MainModel(\n",
    "        categories=tuple(categories_list),\n",
    "        num_continuous=num_continuous,\n",
    "        dim=params['dim'],\n",
    "        dim_out=2, # Binário\n",
    "        depth=params['depth'],\n",
    "        heads=params['heads'],\n",
    "        attn_dropout=params['attn_dropout'],\n",
    "        ff_dropout=params['ff_dropout'],\n",
    "        U=params['U'],\n",
    "        cases=params['cases']\n",
    "    )\n",
    "\n",
    "    # Wrapper para Keras4Torch (Num_Cat)\n",
    "    full_model = Num_Cat(base_model, num_number=num_continuous, classes=2, Sample_size=params['dim'])\n",
    "    \n",
    "    # Construção do Modelo\n",
    "    model = keras4torch.Model(full_model).build([num_continuous, len(categories_list)])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=F.cross_entropy, metrics=['accuracy'])\n",
    "\n",
    "    # Critério de Parada (Patience=20 conforme PDF)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "    \n",
    "    # Treinamento (verbose=0 para não poluir o output)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, # Limite seguro para o Optuna\n",
    "        batch_size=params['batch_size'],\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[es],\n",
    "        verbose=0 \n",
    "    )\n",
    "    \n",
    "    # Retorna o melhor loss de validação\n",
    "    return es.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Iniciando Otimização de Hiperparâmetros (Optuna) ---\")\n",
    "# Cria o estudo para MINIMIZAR a perda (loss)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Executa 20 tentativas (trials) - Ajuste se tiver tempo/GPU sobrando\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"\\n--- Otimização Concluída ---\")\n",
    "print(\"Melhores Hiperparâmetros encontrados:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p = study.best_params\n",
    "\n",
    "print(f\"Treinando modelo final com: {best_p}\")\n",
    "\n",
    "# Recria a arquitetura com os melhores parâmetros\n",
    "final_base = MainModel(\n",
    "    categories=tuple(categories_list),\n",
    "    num_continuous=num_continuous,\n",
    "    dim=best_p['dim'],\n",
    "    dim_out=2,\n",
    "    depth=best_p['depth'],\n",
    "    heads=best_p['heads'],\n",
    "    attn_dropout=best_p['attn_dropout'],\n",
    "    ff_dropout=best_p['ff_dropout'],\n",
    "    U=best_p['U'],\n",
    "    cases=best_p['cases']\n",
    ")\n",
    "\n",
    "final_wrapper = Num_Cat(final_base, num_number=num_continuous, classes=2, Sample_size=best_p['dim'])\n",
    "final_model = keras4torch.Model(final_wrapper).build([num_continuous, len(categories_list)])\n",
    "\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=best_p['lr'], weight_decay=best_p['weight_decay'])\n",
    "final_model.compile(optimizer=optimizer, loss=F.cross_entropy, metrics=['accuracy'])\n",
    "\n",
    "# Callbacks para o modelo final\n",
    "# Salva o melhor modelo da história do treinamento\n",
    "cp = ModelCheckpoint('best_stab_model.pt', monitor='val_loss', save_best_only=True)\n",
    "es_final = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=500, # Mais épocas para o modelo final\n",
    "    batch_size=best_p['batch_size'],\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[es_final, cp],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Carrega os pesos da melhor época\n",
    "final_model.load_weights('best_stab_model.pt')\n",
    "print(\"Melhor modelo carregado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predições no Conjunto de Teste ---\n",
    "logits = final_model.predict(X_test)\n",
    "# Aplica Softmax para ter probabilidades\n",
    "probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "y_pred_class = np.argmax(probs, axis=1)\n",
    "y_prob_churn = probs[:, 1] # Probabilidade da classe 1 (Churn)\n",
    "\n",
    "# 1. Relatório de Classificação\n",
    "print(\"\\n=== Relatório de Classificação (Teste) ===\")\n",
    "print(classification_report(y_test.numpy(), y_pred_class))\n",
    "\n",
    "# 2. Cálculo do KS (Kolmogorov-Smirnov) - MÉTRICA PRINCIPAL\n",
    "class_0_probs = y_prob_churn[y_test.numpy() == 0]\n",
    "class_1_probs = y_prob_churn[y_test.numpy() == 1]\n",
    "\n",
    "ks_stat, p_val = ks_2samp(class_0_probs, class_1_probs)\n",
    "\n",
    "print(f\"\\n=== Métrica KS (Kolmogorov-Smirnov) ===\")\n",
    "print(f\"KS Statistic: {ks_stat:.4f}\")\n",
    "print(f\"P-value: {p_val}\")\n",
    "\n",
    "if ks_stat > 0.4:\n",
    "    print(\">> Resultado: BOM/MUITO BOM (KS > 0.4)\")\n",
    "else:\n",
    "    print(\">> Resultado: REGULAR (KS < 0.4)\")\n",
    "\n",
    "# 3. Gráfico da Curva KS (Opcional, mas recomendado no PDF)\n",
    "def plot_ks_curve(y_true, y_probs):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Ordena probabilidades\n",
    "    thresholds = np.sort(y_probs)\n",
    "    \n",
    "    # Calcula CDFs empíricas\n",
    "    tpr = [] # True Positive Rate (acumulado classe 1)\n",
    "    fpr = [] # False Positive Rate (acumulado classe 0)\n",
    "    \n",
    "    n_pos = np.sum(y_true)\n",
    "    n_neg = len(y_true) - n_pos\n",
    "    \n",
    "    # Truque rápido para plotar KS\n",
    "    for th in thresholds:\n",
    "        tp = np.sum((y_probs >= th) & (y_true == 1))\n",
    "        fp = np.sum((y_probs >= th) & (y_true == 0))\n",
    "        tpr.append(tp / n_pos)\n",
    "        fpr.append(fp / n_neg)\n",
    "        \n",
    "    plt.plot(thresholds, tpr, label='Classe 1 (Churn)')\n",
    "    plt.plot(thresholds, fpr, label='Classe 0 (Não Churn)')\n",
    "    plt.title(f'Curva KS - Estatística: {ks_stat:.4f}')\n",
    "    plt.xlabel('Threshold (Probabilidade)')\n",
    "    plt.ylabel('Proporção Acumulada')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_ks_curve(y_test.numpy(), y_prob_churn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
